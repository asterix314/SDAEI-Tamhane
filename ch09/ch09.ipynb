{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0c714a-801d-4de5-96d7-e375584311c7",
   "metadata": {},
   "source": [
    "# Chapter 9 Inferences for Proportions and Count Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77880b95-9144-4d43-9119-3197c2ea4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from polars import col, lit\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97c34b-0e54-4351-97c3-0485d2d5d74e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9.1 Inferences on Proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bef62-84fa-4cbf-aa52-aa47f83ab4a1",
   "metadata": {},
   "source": [
    "This chapter begins with inference procedures for an unknown proportion $p$ in a Bernoulli population. The sample proportion $\\hat{p}$ from a random sample of size $n$ is an unbiased estimate of $p$. Inferences on p are based on the central limit theorem (CLT) result that for large $n$, the sample proportion $\\hat{p}$ is approximately normal with mean = $p$ and standard deviation = $\\sqrt{pq/n}$ . A large sample two-sided 100(1- $\\alpha$)% confidence interval for $p$ is given by\n",
    "\n",
    "$$\n",
    "\\left[ \\hat{p} \\pm z_{\\alpha /2} \\sqrt{\\frac{\\hat{p} \\hat{q}}{n}}\\;\\right]\n",
    "$$\n",
    "\n",
    "where $\\hat{q}$ = 1 - $\\hat{p}$ and $z_{\\alpha/2}$ is the upper $\\alpha/2$ critical point of the standard normal distribution. A large sample test on $p$ to test $H_0: p = p_0$ can be based on the test statistic\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{p} - p_0}{\\sqrt{\\hat{p}\\hat{q}/n}} \\quad \\text{or} \\quad \n",
    "z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0 q_0 / n}}.\n",
    "$$\n",
    "\n",
    "Both these statistics are asymptotically standard normal under $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fd2c6-a891-4d89-b524-44e7bde59409",
   "metadata": {},
   "source": [
    "### Ex 10.1\n",
    "\n",
    "Tell whether the following mathematical models are theoretical and deterministic or empirical and probabilistic.\n",
    "\n",
    "1. Maxwell's equations of electromagnetism. ✍️ theoretical / deterministic\n",
    "2. An econometric model of the U.S. economy. ✍️ empirical / probabilistic\n",
    "3. A credit scoring model for the probability of a credit applicant being a good risk as a function of selected variables, e.g., income, outstanding debts, etc. ✍️ empirical / probabilistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ead40d-9985-4da8-bbf2-824531356006",
   "metadata": {},
   "source": [
    "### Ex 10.2\n",
    "\n",
    "Tell whether the following mathematical models are theoretical and deterministic or empirical and probabilistic.\n",
    "\n",
    "1. An item response model for the probability of a correct response to an item on a \"true-false\" test as a function of the item's intrinsic difficulty.  ✍️ empirical / probabilistic\n",
    "2. The Cobb-Douglas production function, which relates the output of a firm to its capital and labor inputs. ✍️ empirical / probabilistic\n",
    "3. Kepler's laws of planetary motion. ✍️ theoretical / deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa19ce1-6ea4-4d0b-a154-1cd1295b9078",
   "metadata": {},
   "source": [
    "### Ex 10.3\n",
    "\n",
    "Give an example of an experimental study in which the explanatory variable is controlled at fixed values, while the response variable is random. Also, give an example of an observational study in which both variables are uncontrolled and random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b25caf-0b9c-46e2-a629-edc56fadb6d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c9d62d7-575a-418c-9c88-4c89dca84bad",
   "metadata": {},
   "source": [
    "## 9.2 Inferences for Comparing Two Proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec0625-eba7-473e-a4b9-9f063270ce1c",
   "metadata": {},
   "source": [
    "Next we consider the problem of comparing two Bernoulli proportions, $p_1$ and $p_2$, based on two independent random samples of sizes $n_1$ and $n_2$. The basis for inferences on $p_1 - p_2$ is the result that for large $n_1$ and $n_2$, the difference in the sample proportions, $\\hat{p}_1 - \\hat{p}_2$, is approximately normal with mean = $p_1 - p_2$ and standard deviation = $\\sqrt{p_1 q_1 / n_1 + p_2 q_2 / n_2}$ . A large sample two- sided 100(1 - $\\alpha$)% confidence interval for $p_1 - p_2$ is given by\n",
    "\n",
    "$$\n",
    "\\left[ \\hat{p}_1 - \\hat{p}_2 \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_1 \\hat{q}_1}{n_1} + \\frac{\\hat{p}_2 \\hat{q}_2}{n_2}}\\; \\right].\n",
    "$$\n",
    "\n",
    "A large sample two-sided $z$-test can be used to test $H_0: p_1 = p_2$ vs. $H_1: p_1 \\ne  p_2$ by using the test statistic\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\frac{\\hat{p}_1 \\hat{q}_1}{n_1} + \\frac{\\hat{p}_2 \\hat{q}_2}{n_2}}}\n",
    "\\quad \\text{or} \\quad \n",
    "z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}\\hat{q}\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\hat{p} = (n_1\\hat{p}_1 + n_2\\hat{p}_2)/(n_1 + n_2)$ is the pooled sample proportion. Small sample tests to compare $p_1$ and $p_2$ are also given for independent samples (Fisher's exact test) and matched pairs designs (McNemar's test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac58ea-dcd1-4b9e-8134-7acd971d4791",
   "metadata": {},
   "source": [
    "## 9.3 Inferences for One-way Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a4bdf-5a30-471d-bd18-d83d4e3c5190",
   "metadata": {},
   "source": [
    "A generalization of the test on the binomial proportion $p$ is a test on the cell probabilities of a multinomial distribution. Based on a random sample of size $n$ from a $c$-cell multinomial distribution (one-way count data) with cell probabilities $p_1, p_2, \\ldots, p_c$, the test of\n",
    "\n",
    "$$\n",
    "H_0: p_1 = p_{10},\\, p_2 = p_{20},\\, \\ldots, \\, p_c = p_{c0} \\quad \\text{vs.} \\quad\n",
    "H_1: \\text{At least one}\\, p_i \\ne p_{i0}\n",
    "$$\n",
    "\n",
    "is based on the **chi-square statistic** having the general form:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n",
    "$$\n",
    "\n",
    "where \"observed\" refers to the observed cell counts $n_i$ and \"expected\" refers to the expected cell counts $e_i = n p_{i0}$ under $H_0$. The degrees of freedom (d.f.) of the chi-square statistic are $c$ - 1. The primary use of this statistic is for the **goodness of fit** test of a specified distribution to a set of data. If any parameters of the distribution are estimated from the data, then one d.f. is deducted for each independent estimated parameter from the total d.f. $c$ - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84416064-4901-46c1-b4bb-400f2ac09253",
   "metadata": {},
   "source": [
    "## 9.4 Inferences for Two-way Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6551d-c13f-4a86-9a01-ea5ef1eadb1f",
   "metadata": {},
   "source": [
    "Two-way count data result when\n",
    "\n",
    "1. a single sample is cross-classified based on two categorical variables into $r$ rows and $c$ columns (**multinomial sampling**), or \n",
    "2. independent samples are drawn from $r$ multinomial distributions with the same $c$ categories\n",
    "(**product multinomial sampling**). \n",
    "\n",
    "In both cases, the data are summarized in the form of an $r \\times c$ **contingency table** of counts. In case (1), the null hypothesis of interest is the **independence hypothesis** between the row and column variables; in case (2), it is the **homogeneity hypothesis**. In both cases, the chi-square statistic has the same general form given above, with the expected count for the $(i, j)$th cell (under $H_0$) being the $i$th row total times the proportion of all observations falling in the $j$ th column. The d.f. of the chi-square statistic equal $(r - 1)(c - 1)$. Thus association between the row and the column variable is demonstrated at level $\\alpha$ if $\\chi^2 > \\chi^2_{(r-1)(c-1), \\alpha}$·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fa88d-a258-4591-abe0-e4614ce8d7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
